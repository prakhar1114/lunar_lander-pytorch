{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2926c390f05ffdd6b94b3d79b5c0b494",
     "grade": false,
     "grade_id": "cell1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 2 - Implement your agent\n",
    "\n",
    "Welcome to Course 4, Programming Assignment 2! We have learned about reinforcement learning algorithms for prediction and control in previous courses and extended those algorithms to large state spaces using function approximation. One example of this was in assignment 2 of course 3 where we implemented semi-gradient TD for prediction and used a neural network as the function approximator. In this notebook, we will build a reinforcement learning agent for control, again using a neural network for function approximation. This combination of neural network function approximators and reinforcement learning algorithms, often referred to as Deep RL, is an active area of research and has led to many impressive results (e. g., AlphaGo: https://deepmind.com/research/case-studies/alphago-the-story-so-far).\n",
    "\n",
    "**In this assignment, you will:**\n",
    "  1. Extend the neural network code from assignment 2 of course 3 to output action-values instead of state-values.\n",
    "  2. Write up the Adam algorithm for neural network optimization.\n",
    "  3. Understand experience replay buffers.\n",
    "  4. Implement Softmax action-selection.\n",
    "  5. Build an Expected Sarsa agent by putting all the pieces together.\n",
    "  6. Solve Lunar Lander with your agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d144c4fd87d2731e3fc9234172decb79",
     "grade": false,
     "grade_id": "cell2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Packages\n",
    "- [numpy](www.numpy.org) : Fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) : Library for plotting graphs in Python.\n",
    "- [RL-Glue](http://www.jmlr.org/papers/v10/tanner09a.html), BaseEnvironment, BaseAgent : Library and abstract classes to inherit from  for reinforcement learning experiments.\n",
    "- [LunarLanderEnvironment](https://gym.openai.com/envs/LunarLander-v2/) : An RLGlue environment that wraps a LundarLander environment implementation from OpenAI Gym.\n",
    "- [collections.deque](https://docs.python.org/3/library/collections.html#collections.deque): a double-ended queue implementation. We use deque to implement the experience replay buffer.\n",
    "- [copy.deepcopy](https://docs.python.org/3/library/copy.html#copy.deepcopy): As objects are not passed by value in python, we often need to make copies of mutable objects. copy.deepcopy allows us to make a new object with the same contents as another object. (Take a look at this link if you are interested to learn more: https://robertheaton.com/2014/02/09/pythons-pass-by-object-reference-as-explained-by-philip-k-dick/)\n",
    "- [tqdm](https://github.com/tqdm/tqdm) : A package to display progress bar when running experiments\n",
    "- [os](https://docs.python.org/3/library/os.html): Package used to interface with the operating system. Here we use it for creating a results folder when it does not exist.\n",
    "- [shutil](https://docs.python.org/3/library/shutil.html): Package used to operate on files and folders. Here we use it for creating a zip file of the results folder.\n",
    "- plot_script: Used for plotting learning curves using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "91bddfd590ad8df8bdfb828acc9c8f03",
     "grade": false,
     "grade_id": "cell3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "# Import necessary libraries\n",
    "# DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "from environment import BaseEnvironment\n",
    "from agent import BaseAgent\n",
    "\n",
    "from lunar_lander import LunarLanderEnvironment\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "import os \n",
    "\n",
    "import shutil\n",
    "\n",
    "from plot_script import plot_result\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from policy import Policy\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import gym\n",
    "\n",
    "import pdb\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3528df9f07a2c2a27c11a47d94849931",
     "grade": false,
     "grade_id": "cell4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Section 1: Action-Value Network\n",
    "This section includes the function approximator that we use in our agent, a neural network. In Course 3 Assignment 2, we used a neural network as the function approximator for a policy evaluation problem. In this assignment, we will use a neural network for approximating the action-value function in a control problem. The main difference between approximating a state-value function and an action-value function using a neural network is that in the former the output layer only includes one unit whereas in the latter the output layer includes as many units as the number of actions. \n",
    "\n",
    "In the cell below, you will specify the architecture of the action-value neural network. More specifically, you will specify `self.layer_size` in the `__init__()` function. \n",
    "\n",
    "We have already provided `get_action_values()` and `get_TD_update()` methods. The former computes the action-value function by doing a forward pass and the latter computes the gradient of the action-value function with respect to the weights times the TD error. These `get_action_values()` and `get_TD_update()` methods are similar to the `get_value()` and `get_gradient()` methods that you implemented in Course 3 Assignment 2. The main difference is that in this notebook, they are designed to be applied to batches of states instead of one state. You will later use these functions for implementing the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "61b88349f16b3681a89d0a0b5f7becd6",
     "grade": false,
     "grade_id": "cell6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the cell below to test your implementation of the `__init__()` function for ActionValueNetwork:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d01bed3174f24e2faacde8268f2dd3af",
     "grade": false,
     "grade_id": "cell8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "    layer_sizes: [ 5 20  3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b1d3111fcd72e10d6fae596c1982b11",
     "grade": false,
     "grade_id": "cell9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Section 2: Adam Optimizer\n",
    "\n",
    "In this assignment, you will use the Adam algorithm for updating the weights of your action-value network. As you may remember from Course 3 Assignment 2, the Adam algorithm is a more advanced variant of stochastic gradient descent (SGD). The Adam algorithm improves the SGD update with two concepts: adaptive vector stepsizes and momentum. It keeps running estimates of the mean and second moment of the updates, denoted by $\\mathbf{m}$ and $\\mathbf{v}$ respectively:\n",
    "$$\\mathbf{m_t} = \\beta_m \\mathbf{m_{t-1}} + (1 - \\beta_m)g_t \\\\\n",
    "\\mathbf{v_t} = \\beta_v \\mathbf{v_{t-1}} + (1 - \\beta_v)g^2_t\n",
    "$$\n",
    "\n",
    "Here, $\\beta_m$ and $\\beta_v$ are fixed parameters controlling the linear combinations above and $g_t$ is the update at time $t$ (generally the gradients, but here the TD error times the gradients).\n",
    "\n",
    "Given that $\\mathbf{m}$ and $\\mathbf{v}$ are initialized to zero, they are biased toward zero. To get unbiased estimates of the mean and second moment, Adam defines $\\mathbf{\\hat{m}}$ and $\\mathbf{\\hat{v}}$ as:\n",
    "$$ \\mathbf{\\hat{m}_t} = \\frac{\\mathbf{m_t}}{1 - \\beta_m^t} \\\\\n",
    "\\mathbf{\\hat{v}_t} = \\frac{\\mathbf{v_t}}{1 - \\beta_v^t}\n",
    "$$\n",
    "\n",
    "The weights are then updated as follows:\n",
    "$$ \\mathbf{w_t} = \\mathbf{w_{t-1}} + \\frac{\\alpha}{\\sqrt{\\mathbf{\\hat{v}_t}}+\\epsilon} \\mathbf{\\hat{m}_t}\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ is the step size parameter and $\\epsilon$ is another small parameter to keep the denominator from being zero.\n",
    "\n",
    "In the cell below, you will implement the `__init__()` and `update_weights()` methods for the Adam algorithm. In `__init__()`, you will initialize `self.m` and `self.v`. In `update_weights()`, you will compute new weights given the input weights and an update $g$ (here `td_errors_times_gradients`) according to the equations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3e71e0d2337c3a571caddde38eed8b8e",
     "grade": false,
     "grade_id": "cell11",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following code to test your implementation of the `__init__()` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "85dd186066f2dd85643cdc1fa6969a9d",
     "grade": false,
     "grade_id": "cell13",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "    m[0][\"W\"] shape: (5, 2)\n",
    "    m[0][\"b\"] shape: (1, 2)\n",
    "    m[1][\"W\"] shape: (2, 3)\n",
    "    m[1][\"b\"] shape: (1, 3) \n",
    "\n",
    "    v[0][\"W\"] shape: (5, 2)\n",
    "    v[0][\"b\"] shape: (1, 2)\n",
    "    v[1][\"W\"] shape: (2, 3)\n",
    "    v[1][\"b\"] shape: (1, 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "959fb7067515ac663cc0b3a6e35ed588",
     "grade": false,
     "grade_id": "cell14",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following code to test your implementation of the `update_weights()` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1f0f3e5d39449a6b242db88e6b03e066",
     "grade": false,
     "grade_id": "cell17",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Section 3: Experience Replay Buffers\n",
    "\n",
    "In Course 3, you implemented agents that update value functions once for each sample. We can use a more efficient approach for updating value functions. You have seen an example of an efficient approach in Course 2 when implementing Dyna. The idea behind Dyna is to learn a model using sampled experience, obtain simulated experience from the model, and improve the value function using the simulated experience.\n",
    "\n",
    "Experience replay is a simple method that can get some of the advantages of Dyna by saving a buffer of experience and using the data stored in the buffer as a model. This view of prior data as a model works because the data represents actual transitions from the underlying MDP. Furthermore, as a side note, this kind of model that is not learned and simply a collection of experience can be called non-parametric as it can be ever-growing as opposed to a parametric model where the transitions are learned to be represented with a fixed set of parameters or weights.\n",
    "\n",
    "We have provided the implementation of the experience replay buffer in the cell below. ReplayBuffer includes two main functions: `append()` and `sample()`. `append()` adds an experience transition to the buffer as an array that includes the state, action, reward, terminal flag (indicating termination of the episode), and next_state. `sample()` gets a batch of experiences from the buffer with size `minibatch_size`.\n",
    "\n",
    "You will use the `append()` and `sample()` functions when implementing the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1) # set random seed\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "network = Policy(s_size=8, h_size=256, a_size=4).to(device)\n",
    "\n",
    "# Remove the following 3 lines to train the agent from scratch\n",
    "model_path = 'action_value_network/action_value_network_300.pth'  # model to load\n",
    "checkpoint = torch.load(model_path)\n",
    "network.load_state_dict(checkpoint)\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001,betas = [0.99,0.999],eps = 1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "002f731f1fd67e1d88ba22a40c852094",
     "grade": false,
     "grade_id": "cell18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not modify this cell! \n",
    "\n",
    "# Work Required: No. However, do go through the code to ensure your understanding is correct.\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size, seed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "            seed (integer): The seed for the random number generator. \n",
    "        \"\"\"\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        self.max_size = size\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "#         self.buffer.append([state, action, reward, terminal, next_state])\n",
    "#         self.buffer.append((torch.tensor(state), torch.tensor(action).to(device), torch.tensor(reward).float().to(device),torch.tensor(terminal).to(device), torch.tensor(next_state)))\n",
    "        self.buffer.append((state, torch.tensor(action).to(device), torch.tensor(reward).float().to(device),torch.tensor(terminal).to(device), next_state))\n",
    "    \n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "        \"\"\"\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "78e00022f5e617732b6527f3523724e0",
     "grade": false,
     "grade_id": "cell21",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the cell below to test your implementation of the `softmax()` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a551ed093fa021ec1e222fb06539a63f",
     "grade": false,
     "grade_id": "cell23",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "    action_probs [[0.25849645 0.01689625 0.05374514 0.67086216]\n",
    "     [0.84699852 0.00286345 0.13520063 0.01493741]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e66eb8fa7dda54905f0b5a45aea2fc88",
     "grade": false,
     "grade_id": "cell24",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Section 5: Putting the pieces together\n",
    "\n",
    "In this section, you will combine components from the previous sections to write up an RL-Glue Agent. The main component that you will implement is the action-value network updates with experience sampled from the experience replay buffer.\n",
    "\n",
    "At time $t$, we have an action-value function represented as a neural network, say $Q_t$. We want to update our action-value function and get a new one we can use at the next timestep. We will get this $Q_{t+1}$ using multiple replay steps that each result in an intermediate action-value function $Q_{t+1}^{i}$ where $i$ indexes which replay step we are at.\n",
    "\n",
    "In each replay step, we sample a batch of experiences from the replay buffer and compute a minibatch Expected-SARSA update. Across these N replay steps, we will use the current \"un-updated\" action-value network at time $t$, $Q_t$, for computing the action-values of the next-states. This contrasts using the most recent action-values from the last replay step $Q_{t+1}^{i}$. We make this choice to have targets that are stable across replay steps. Here is the pseudocode for performing the updates:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& Q_t \\leftarrow \\text{action-value network at timestep t (current action-value network)}\\\\\n",
    "& \\text{Initialize } Q_{t+1}^1 \\leftarrow Q_t\\\\\n",
    "& \\text{For } i \\text{ in } [1, ..., N] \\text{ (i.e. N} \\text{  replay steps)}:\\\\\n",
    "& \\hspace{1cm} s, a, r, t, s'\n",
    "\\leftarrow \\text{Sample batch of experiences from experience replay buffer} \\\\\n",
    "& \\hspace{1cm} \\text{Do Expected Sarsa update with } Q_t: Q_{t+1}^{i+1}(s, a) \\leftarrow Q_{t+1}^{i}(s, a) + \\alpha \\cdot \\left[r + \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right) - Q_{t+1}^{i}(s, a)\\right]\\\\\n",
    "& \\hspace{1.5cm} \\text{ making sure to add the } \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right) \\text{ for non-terminal transitions only.} \\\\\n",
    "& \\text{After N replay steps, we set } Q_{t+1}^{N} \\text{ as } Q_{t+1} \\text{ and have a new } Q_{t+1} \\text{for time step } t + 1 \\text{ that we will fix in the next set of updates. }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As you can see in the pseudocode, after sampling a batch of experiences, we do many computations. The basic idea however is that we are looking to compute a form of a TD error. In order to so, we can take the following steps:\n",
    "- compute the action-values for the next states using the action-value network $Q_{t}$,\n",
    "- compute the policy $\\pi(b | s')$ induced by the action-values $Q_{t}$ (using the softmax function you implemented before),\n",
    "- compute the Expected sarsa targets $r + \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right)$,\n",
    "- compute the action-values for the current states using the latest $Q_{t + 1}$, and,\n",
    "- compute the TD-errors with the Expected Sarsa targets.\n",
    " \n",
    "For the third step above, you can start by computing $\\pi(b | s') Q_t(s', b)$ followed by summation to get $\\hat{v}_\\pi(s') = \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right)$. $\\hat{v}_\\pi(s')$ is an estimate of the value of the next state. Note for terminal next states, $\\hat{v}_\\pi(s') = 0$. Finally, we add the rewards to the discount times $\\hat{v}_\\pi(s')$.\n",
    "\n",
    "You will implement these steps in the `get_td_error()` function below which given a batch of experiences (including states, next_states, actions, rewards, terminals), fixed action-value network (current_q), and action-value network (network), computes the TD error in the form of a 1D array of size batch_size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7efa3ae7896324abe10eff83fa20c3a1",
     "grade": false,
     "grade_id": "cell26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following code to test your implementation of the `get_td_error()` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ea7393d98a54b3f73abf2177a499e83",
     "grade": false,
     "grade_id": "cell28",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now that you implemented the `get_td_error()` function, you can use it to implement the `optimize_network()` function. In this function, you will:\n",
    "- get the TD-errors vector from `get_td_error()`,\n",
    "- make the TD-errors into a matrix using zeroes for actions not taken in the transitions,\n",
    "- pass the TD-errors matrix to the `get_TD_update()` function of network to calculate the gradients times TD errors, and,\n",
    "- perform an ADAM optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences, discount, current_model, tau):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the TD-error and update the network\n",
    "    \n",
    "    \"\"\"\n",
    "    model=network\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "    \n",
    "#     print(next_states)\n",
    "    q_next = current_model.get_action_values(torch.stack(next_states)).squeeze()\n",
    "    probs = current_model.forward(torch.stack(next_states)).squeeze()\n",
    "\n",
    "    # calculate the maximum action value of next states\n",
    "#     expected_q_next = (1-torch.stack(terminals)) * (torch.sum(probs * q_next , axis = 1))\n",
    "    max_q_next = (1-torch.stack(terminals)) * (torch.max(q_next , axis = 1)[0])\n",
    "    # calculate the targets  0%|          | 0/600 [00:00<?, ?it/s]<ipython-input-24-1f85acab5351>:30:\n",
    "    \n",
    "    rewards = torch.stack(rewards)\n",
    "#     targets = Variable(rewards + (discount * expected_q_next)).float()\n",
    "    targets = (rewards + (discount * max_q_next))\n",
    "    \n",
    "    # calculate the outputs from the previous states (batch_size, num_actions)\n",
    "    outputs = network.get_action_values(torch.stack(states)).squeeze()\n",
    "    \n",
    "    actions = torch.stack(actions).view(-1,1)\n",
    "    \n",
    "    outputs = torch.gather(outputs, 1, actions).squeeze()\n",
    "    \n",
    "    # the loss\n",
    "#     pdb.set_trace()  \n",
    "    loss = torch.sum((targets-outputs)**2,axis=0)/rewards.shape[0]\n",
    "       \n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3889eb0e75da92c21d961ec401627989",
     "grade": false,
     "grade_id": "cell30",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following code to test your implementation of the `optimize_network()` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b83a775d274b1dc0d9ee3e3945fcef8",
     "grade": false,
     "grade_id": "cell32",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now that you implemented the `optimize_network()` function, you can implement the agent. In the cell below, you will fill the `agent_step()` and `agent_end()` functions. You should:\n",
    "- select an action (only in `agent_step()`),\n",
    "- add transitions (consisting of the state, action, reward, terminal, and next state) to the replay buffer, and,\n",
    "- update the weights of the neural network by doing multiple replay steps and calling the `optimize_network()` function that you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7da1605d44c8ca965b334815ca9ea065",
     "grade": false,
     "grade_id": "cell33",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### Work Required: Yes. Fill in code in agent_step and agent_end (~7 Lines).\n",
    "class Agent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        self.name = \"expected_sarsa_agent\"\n",
    "        \n",
    "    # Work Required: No.\n",
    "    def agent_init(self, agent_config):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Set parameters needed to setup the agent.\n",
    "\n",
    "        Assume agent_config dict contains:\n",
    "        {\n",
    "            network_config: dictionary,\n",
    "            optimizer_config: dictionary,\n",
    "            replay_buffer_size: integer,\n",
    "            minibatch_sz: integer, \n",
    "            num_replay_updates_per_step: float\n",
    "            discount_factor: float,\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
    "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"))\n",
    "        \n",
    "\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.tau = agent_config['tau']\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        \n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "\n",
    "    # Work Required: No.\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): the state.\n",
    "        Returns:\n",
    "            the action. \n",
    "        \"\"\"\n",
    "#         action_values = self.network.get_action_values(state)\n",
    "#         probs_batch = softmax(action_values, self.tau)\n",
    "#         action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
    "        action = network.act(state)\n",
    "#         print(action)\n",
    "        return action\n",
    "\n",
    "    # Work Required: No.\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "#         self.last_state = np.array([state])\n",
    "        state = torch.tensor([state]).view(1, -1)\n",
    "        self.last_state = state.to(device)\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "    # Work Required: Yes. Fill in the action selection, replay-buffer update, \n",
    "    # weights update using optimize_network, and updating last_state and last_action (~5 lines).\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "\n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "#         state = np.array([state])\n",
    "        state = torch.tensor([state]).view(1, -1).to(device)\n",
    "\n",
    "        # Select action\n",
    "        ### START CODE HERE (~1 Line)\n",
    "        action = self.policy(state)\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        # Append new experience to replay buffer\n",
    "        # Note: look at the replay_buffer append function for the order of arguments\n",
    "\n",
    "        ### START CODE HERE (~1 Line)\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward,  0, state)\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(network).to(device)\n",
    "#             for paramss in current_q.parameters():\n",
    "#                 paramss.requires_grad = False     \n",
    "#             current_q.eval()                \n",
    "                \n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network (~1 Line)\n",
    "                ### START CODE HERE\n",
    "                optimize_network(experiences, self.discount, current_q, self.tau)\n",
    "\n",
    "                ### END CODE HERE\n",
    "                \n",
    "        # Update the last state and last action.\n",
    "        ### START CODE HERE (~2 Lines)\n",
    "        self.last_state = state\n",
    "        self.last_action = int(action)\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return action\n",
    "\n",
    "    # Work Required: Yes. Fill in the replay-buffer update and\n",
    "    # update of the weights using optimize_network (~2 lines).\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Set terminal state to an array of zeros\n",
    "#         state = np.zeros_like(self.last_state)\n",
    "        state = torch.zeros_like(self.last_state).to(device)\n",
    "\n",
    "        # Append new experience to replay buffer\n",
    "        # Note: look at the replay_buffer append function for the order of arguments\n",
    "        \n",
    "        ### START CODE HERE (~1 Line)\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 1, state)\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(network)\n",
    "#             for paramss in current_q.parameters():\n",
    "#                 paramss.requires_grad = False\n",
    "                \n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network\n",
    "                ### START CODE HERE (~1 Line)\n",
    "                optimize_network(experiences, self.discount, current_q, self.tau)\n",
    "                               \n",
    "                ### END CODE HERE\n",
    "                \n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dab79d391aef354cd1fc454690c534d3",
     "grade": false,
     "grade_id": "cell34",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following code to test your implementation of the `agent_step()` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "31468a23f769e117fd042f1be96c3cff",
     "grade": false,
     "grade_id": "cell36",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following code to test your implementation of the `agent_end()` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dcbf680f9de1cca0814b50b4a728c47b",
     "grade": false,
     "grade_id": "cell38",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Section 6: Run Experiment\n",
    "\n",
    "Now that you implemented the agent, we can use it to run an experiment on the Lunar Lander problem. We will plot the learning curve of the agent to visualize learning progress. To plot the learning curve, we use the sum of rewards in an episode as the performance measure. We have provided for you the experiment/plot code in the cell below which you can go ahead and run. Note that running the cell below has taken approximately 10 minutes in prior testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "228645061594e9fa701c8efe0081ad8e",
     "grade": false,
     "grade_id": "cell39",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_sum 209.860415: 100%|██████████| 500/500 [53:28<00:00,  6.42s/it] \n"
     ]
    }
   ],
   "source": [
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save sum of reward at the end of each episode\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "\n",
    "    env_info = {}\n",
    "\n",
    "    agent_info = agent_parameters\n",
    "\n",
    "    # one agent setting\n",
    "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info[\"network_config\"][\"seed\"] = run\n",
    "        env_info[\"seed\"] = run\n",
    "\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "        with trange(1, experiment_parameters[\"num_episodes\"]+1) as t:\n",
    "            for episode in t:\n",
    "        \n",
    "#         for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
    "                # run episode\n",
    "                rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "\n",
    "                episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "                agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "                t.set_description('episode_sum %f' % episode_reward)\n",
    "#                 t.set_postfix(\"episode_reward = %i\" % episode_reward)\n",
    "#                 sleep(0.1)\n",
    "                \n",
    "#                 if episode%10==0\n",
    "#                     print(\"Episode Reward sum\", episode_reward)\n",
    "    \n",
    "    \n",
    "    torch.save(network.state_dict(),'action_value_network/action_value_network_{}.pth'.format(experiment_parameters[\"num_episodes\"]))\n",
    "    \n",
    "    save_name = \"{}\".format(rl_glue.agent.name)\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
    "    shutil.make_archive('results', 'zip', 'results')\n",
    "\n",
    "# Run Experiment\n",
    "\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 1,\n",
    "    \"num_episodes\" : 500,\n",
    "    # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after \n",
    "    # some number of timesteps. Here we use the default of 1000.\n",
    "    \"timeout\" : 1000\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = {}\n",
    "\n",
    "current_env = LunarLanderEnvironment\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {\n",
    "    'network_config': {\n",
    "        'state_dim': 8,\n",
    "        'num_hidden_units': 256,\n",
    "        'num_actions': 4\n",
    "    },\n",
    "    'optimizer_config': {\n",
    "        'step_size': 1e-3,\n",
    "        'beta_m': 0.9, \n",
    "        'beta_v': 0.999,\n",
    "        'epsilon': 1e-8\n",
    "    },\n",
    "    'replay_buffer_size': 50000,\n",
    "    'minibatch_sz': 8,\n",
    "    'num_replay_updates_per_step': 4,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.001\n",
    "}\n",
    "current_agent = Agent\n",
    "\n",
    "# run experiment\n",
    "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5e6d81cab5de679eba86a658acbe9237",
     "grade": false,
     "grade_id": "cell40",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the cell below to see the comparison between the agent that you implemented and a random agent for the one run and 300 episodes. Note that the `plot_result()` function smoothes the learning curve by applying a sliding window on the performance measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5390f62315c8303c9e54b45b7e5a8d69",
     "grade": false,
     "grade_id": "cell41",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result([\"expected_sarsa_agent\", \"random_agent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "206b2c64f0e560a6dbfc71a9f578a2fb",
     "grade": false,
     "grade_id": "cell42",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In the following cell you can visualize the performance of the agent with a correct implementation. As you can see, the agent initially crashes quite quickly (Episode 0). Then, the agent learns to avoid crashing by expending fuel and staying far above the ground. Finally however, it learns to land smoothly within the landing zone demarcated by the two flags (Episode 275)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "952d4cf4a5f9c52431cba64a007735c7",
     "grade": false,
     "grade_id": "cell43",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\">\n",
       "<video width=\"80%\" controls>\n",
       "      <source src=\"ImplementYourAgent.mp4\" type=\"video/mp4\">\n",
       "</video></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"ImplementYourAgent.mp4\" type=\"video/mp4\">\n",
    "</video></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1) # set random seed\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "network = Policy(s_size=8, h_size=256, a_size=4).to(device)\n",
    "\n",
    "model_path = 'action_value_network/action_value_network_500.pth'  # model to load\n",
    "checkpoint = torch.load(model_path)\n",
    "network.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward obtained in this episode is 109.41931301243477\n",
      "Total reward obtained in this episode is 120.15328935951902\n",
      "Total reward obtained in this episode is 171.14828142129022\n",
      "Total reward obtained in this episode is 116.41236106887\n",
      "Total reward obtained in this episode is 109.07608108539932\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-58e0f679a39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0msum_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum_reward\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "num_episodes = 20\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state=env.reset()\n",
    "    state=torch.from_numpy(state).to(device).view(1, -1)\n",
    "    is_terminal=False \n",
    "    env.render()\n",
    "    sum_reward=0\n",
    "    \n",
    "    while not is_terminal: \n",
    "        action = network.act(state)\n",
    "        current_state, reward, is_terminal, _ = env.step(action)\n",
    "        env.render()\n",
    "        state=torch.from_numpy(current_state).to(device).view(1, -1)\n",
    "        time.sleep(0.02)\n",
    "        sum_reward=sum_reward+reward\n",
    "    \n",
    "    print(\"Total reward obtained in this episode is {}\".format(sum_reward))\n",
    "    env.close()   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "complete-reinforcement-learning-system",
   "graded_item_id": "8dMlx",
   "launcher_item_id": "4O5gG"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
